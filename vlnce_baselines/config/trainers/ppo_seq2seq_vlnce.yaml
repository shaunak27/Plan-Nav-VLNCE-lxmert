# ------------------------------------------------------------------------------
# Experiment configuration
# ------------------------------------------------------------------------------
BASE_TASK_CONFIG_PATH: habitat_extensions/config/vlnce_rl_global_task.yaml

USE_CPU: False
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
NUM_PROCESSES: 1

TENSORBOARD_DIR: data/out/ppo/tensorboard_dirs/seq2seq_vlnce
EVAL_CKPT_PATH_DIR: data/out/ppo/checkpoints/seq2seq_vlnce
CHECKPOINT_FOLDER: data/out/ppo/checkpoints/seq2seq_vlnce
LOAD_FROM_CKPT: False
LOAD_CKPT_FROM: data/out/ppo/checkpoints/seq2seq_vlnce/ckpt.0.pth
CHECKPOINT_INTERVAL: 250
LOG_INTERVAL: 50
STATS_FILE: data/out/ppo/stats/seq2seq_vlnce
LOG_FILE: "ppo_seq2seq_vlnce.log"
VIDEO_DIR: data/out/ppo/videos/seq2seq_vlnce
VIDEO_OPTION: [] # ["disk"]
# ------------------------------------------------------------------------------
# Training details
# ------------------------------------------------------------------------------
NUM_UPDATES: 10000
TRAINER_NAME: "vlnce_ppo" 
ENV_NAME:  "VLNCEPPORLEnv" 
SENSORS: ["RGB_SENSOR", "DEPTH_SENSOR"]
# ------------------------------------------------------------------------------
# Evaluation details
# ------------------------------------------------------------------------------
EVAL:
  USE_CKPT_CONFIG: False
  SPLIT: val_seen
  EPISODE_COUNT: -1
# ------------------------------------------------------------------------------
# Model details
# ------------------------------------------------------------------------------
MODEL:
  POLICY: "seq2seq"
  SEQ2SEQ:
    use_prev_action: True
  POINTGOAL_ENCODER:
    use: False
  PROGRESS_MONITOR:
    use: False
  INSTRUCTION_ENCODER:
    use_pretrained_embeddings: True
# ------------------------------------------------------------------------------
# RL details
# ------------------------------------------------------------------------------
RL:
  FORWARD_STEP_SIZE: 1.0
  TURN_ANGLE_SIZE: 15
  SUCCESS_MEASURE: "spl"
  SLACK_REWARD: -0.001
  SUCCESS_REWARD: 10.0
  DISTANCE_REWARD:
    COMPUTE: True
    SCALE: 0.5
    MEASURE: "geodesic_distance"
  COLLISION_COUNT_REWARD:
    COMPUTE: False
    MEASURE: "collision_count" 
    THRESH: 5
    REWARD: -0.001
  COLLISION_DISTANCE_REWARD:
    COMPUTE: False
    MEASURE: "collision_distance" 
    THRESH: 0.2
    REWARD: -0.004
  PPO: 
    clip_param: 0.1
    ppo_epoch: 4
    # This was 4 in the paper
    num_mini_batch: 1
    value_loss_coef: 0.5
    entropy_coef: 0.01
    lr: 2.5e-4
    eps: 1.e-5
    max_grad_norm: 0.5
    num_steps: 128
    hidden_size: 512
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: True
    use_linear_lr_decay: True
    reward_window_size: 50
    use_normalized_advantage: False